[
  {
    "objectID": "prescribing.html",
    "href": "prescribing.html",
    "title": "Prescribing example",
    "section": "",
    "text": "Peamble for nice formatting\ninstall.packages(\"../minisparra/\", repos = NULL, type = \"source\")\nlibrary(miniSPARRA01)\n\n# Formatting\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\nkbl &lt;- function(x) {\n  x &lt;- head(x, 10)\n  x %&gt;%\n    knitr::kable(format = 'html', table.attr = 'data-quarto-disable-processing=\"true\"') %&gt;%\n    kableExtra::kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n}"
  },
  {
    "objectID": "prescribing.html#prescribing-data",
    "href": "prescribing.html#prescribing-data",
    "title": "Prescribing example",
    "section": "1. Prescribing data",
    "text": "1. Prescribing data\n\nfilenames &lt;- c(\"../example_data/random_pis_data.csv\")\nall_tables &lt;- read_all_tables(filenames)\nall_tables$`../example_data/random_pis_data.csv` %&gt;% kbl()\n\n\n \n  \n    id \n    paid_date \n    bnf_section \n    num_items \n  \n \n\n  \n    19 \n    2017-12-15 \n    106 \n    3 \n  \n  \n    19 \n    2016-08-11 \n    103 \n    5 \n  \n  \n    19 \n    2015-07-07 \n    103 \n    1 \n  \n  \n    7 \n    2017-03-14 \n    106 \n    1 \n  \n  \n    3 \n    2015-08-07 \n    105 \n    1 \n  \n  \n    18 \n    2016-05-07 \n    103 \n    3 \n  \n  \n    2 \n    2016-09-29 \n    104 \n    4 \n  \n  \n    5 \n    2015-12-01 \n    102 \n    3 \n  \n  \n    2 \n    2017-01-11 \n    105 \n    1 \n  \n  \n    6 \n    2017-11-04 \n    105 \n    4 \n  \n\n\n\n\n\nHere we have a random example prescribing data file. From the first few rows we can see that patient with id 19 had 3 items from under bnf_section 0106 prescribed on 15th December 2017, had 5 items from bnf_section 0103 prescribed oon 11th August 2016, and had 1 item from bnf_section 0103 on 7th July 2015."
  },
  {
    "objectID": "prescribing.html#specification-file---number-of-unique",
    "href": "prescribing.html#specification-file---number-of-unique",
    "title": "Prescribing example",
    "section": "2. Specification file - Number of unique",
    "text": "2. Specification file - Number of unique\n\nwriteLines(readLines(\"../example_spec/pis_data_nunique.json\"))\n\n{\n    \"source_file\": [\"../example_data/random_pis_data.csv\"],\n    \"transformation_type\": [\"NUNIQUE\"],\n    \"aggregation_column\": [\"bnf_section\"],\n    \"grouping_columns\": [\"id\"],\n    \"absent_data_flag\": [0],\n    \"output_feature_name\": [\"user_defined_name\"],\n    \"primary_filter\" : {\n        \"type\": [\"AND\"],\n        \"subfilters\":{\n            \"subfilter_1\": {\n                \"column\": [\"paid_date\"],\n                \"type\": [\"DATE_GT_EQ\"],\n                \"value\": [\"2015-01-01\"]\n            },\n            \"subfilter_2\": {\n                \"column\": [\"paid_date\"],\n                \"type\": [\"DATE_LT_EQ\"],\n                \"value\": [\"2018-01-01\"]\n            }\n        }\n    }\n}\n\n\nThis specification file is for defining the number of unique instances in the bnf_section column (called the “aggregation_column” in the specification) grouped by patient id. So if there were no more entries for patient 19 in the data file, then they would have 2 unique entries. There is date filtering applied. In this case between 1st January 2015 and 1st January 2018 inclusive."
  },
  {
    "objectID": "prescribing.html#run-transformation---number-of-unique",
    "href": "prescribing.html#run-transformation---number-of-unique",
    "title": "Prescribing example",
    "section": "3. Run transformation - Number of unique",
    "text": "3. Run transformation - Number of unique\n\nall_table_filenames &lt;- c(\"../example_data/random_pis_data.csv\")\nall_feature_json_filenames &lt;- c(\"../example_spec/pis_data_nunique.json\")\n\ntf &lt;- transform(\n  all_table_filenames = all_table_filenames,\n  all_feature_json_filenames = all_feature_json_filenames\n)\n\ntf\n\n   id user_defined_name\n1   0                 3\n2   1                 3\n3   2                 4\n4   3                 6\n5   4                 3\n6   5                 5\n7   6                 3\n8   7                 2\n9   8                 7\n10  9                 2\n11 10                 3\n12 11                 1\n13 12                 3\n14 13                 4\n15 14                 3\n16 15                 8\n17 16                 3\n18 17                 6\n19 18                 4\n20 19                 5\n\n\nThe results of the transformation are shown above - here we patient with id 0 had 3 unique bnf_section and patient with id 19 ultimately ended up with 5 unique selections over the specified time period."
  },
  {
    "objectID": "prescribing.html#number-of-prescibed-items",
    "href": "prescribing.html#number-of-prescibed-items",
    "title": "Prescribing example",
    "section": "4. Number of prescibed items",
    "text": "4. Number of prescibed items\nThe following specification can be used to return the number of prescribed items, per patient over a specified date range.\n\nwriteLines(readLines(\"../example_spec/pis_data_nprescribed.json\"))\n\n{\n    \"source_file\": [\"../example_data/random_pis_data.csv\"],\n    \"transformation_type\": [\"SUM\"],\n    \"aggregation_column\": [\"num_items\"],\n    \"grouping_columns\": [\"id\"],\n    \"absent_data_flag\": [0],\n    \"output_feature_name\": [\"number_of_prescribed_items_goes_here\"],\n    \"primary_filter\" : {\n        \"type\": [\"AND\"],\n        \"subfilters\":{\n            \"subfilter_1\": {\n                \"column\": [\"paid_date\"],\n                \"type\": [\"DATE_GT_EQ\"],\n                \"value\": [\"2015-01-01\"]\n            },\n            \"subfilter_2\": {\n                \"column\": [\"paid_date\"],\n                \"type\": [\"DATE_LT_EQ\"],\n                \"value\": [\"2018-01-01\"]\n            }\n        }\n    }\n}\n\n\nWith the corresponding transformation\n\nall_table_filenames &lt;- c(\"../example_data/random_pis_data.csv\")\nall_feature_json_filenames &lt;- c(\"../example_spec/pis_data_nprescribed.json\")\n\ntf_nitems &lt;- transform(\n  all_table_filenames = all_table_filenames,\n  all_feature_json_filenames = all_feature_json_filenames\n)\n\ntf_nitems\n\n   id number_of_prescribed_items_goes_here\n1   0                                   14\n2   1                                   10\n3   2                                   14\n4   3                                   27\n5   4                                   11\n6   5                                   15\n7   6                                    8\n8   7                                   10\n9   8                                   28\n10  9                                   10\n11 10                                   12\n12 11                                    6\n13 12                                    8\n14 13                                   17\n15 14                                    8\n16 15                                   27\n17 16                                    8\n18 17                                   27\n19 18                                   15\n20 19                                   27"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "minisparra",
    "section": "",
    "text": "This Quarto book serves as a demo of the minisparra package (for which we still don’t have a real name).\nFirst, we load the package from the local directory.\ninstall.packages(\"../minisparra/\", repos = NULL, type = \"source\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(miniSPARRA01)\n\n\nAttaching package: 'miniSPARRA01'\n\n\nThe following object is masked from 'package:base':\n\n    transform\nAlso, let’s set up a function to visualise tables nicely.\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::group_rows() masks kableExtra::group_rows()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nkbl &lt;- function(x) {\n  x &lt;- head(x, 10)\n  x %&gt;%\n    knitr::kable(format = 'html', table.attr = 'data-quarto-disable-processing=\"true\"') %&gt;%\n    kableExtra::kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n}"
  },
  {
    "objectID": "index.html#decide-what-you-want-to-filter-for",
    "href": "index.html#decide-what-you-want-to-filter-for",
    "title": "minisparra",
    "section": "0. Decide what you want to filter for",
    "text": "0. Decide what you want to filter for\nMINISPARRA is designed to process (Scottish) medical data and collate using a range of filter functions to allow users to easily extract tables for ML pipelines.\nArgubly the first step should be to decide which data you will use, and how you will filter it. MINISPARRA allows users to specify a range of data sources (files/tables) and to define which features they want to extract data via a specification json. MINISPARRA will filter and join the data as per the spec."
  },
  {
    "objectID": "index.html#read-in-data",
    "href": "index.html#read-in-data",
    "title": "minisparra",
    "section": "1. Read in data",
    "text": "1. Read in data\nIn this example we will use some random A&E-type data stored in a csv file, and filter based on 2 categories. The attendance_type column and the diagnosis_1 column. But data can be derived from multiple sources, and joined subsequently.\nThis step must be done before any feature extraction to avoid having to read the same table multiple times.\n\nfilenames &lt;- c(\"../example_data/random_ae_data.csv\")\nall_tables &lt;- read_all_tables(filenames)\nall_tables$`../example_data/random_ae_data.csv` %&gt;% kbl()\n\n\n \n  \n    id \n    time \n    attendance_category \n    diagnosis_1 \n    diagnosis_2 \n    diagnosis_3 \n  \n \n\n  \n    2 \n    2016-08-16 \n    1 \n    104 \n    103 \n    102 \n  \n  \n    2 \n    2015-07-28 \n    1 \n    101 \n    103 \n    104 \n  \n  \n    16 \n    2017-09-20 \n    1 \n    102 \n    101 \n    104 \n  \n  \n    10 \n    2016-03-25 \n    1 \n    104 \n    103 \n    101 \n  \n  \n    5 \n    2016-09-20 \n    1 \n    103 \n    104 \n    101 \n  \n  \n    6 \n    2016-02-11 \n    1 \n    103 \n    102 \n    101 \n  \n  \n    14 \n    2016-02-14 \n    1 \n    102 \n    103 \n    104 \n  \n  \n    14 \n    2015-11-09 \n    1 \n    104 \n    103 \n    102 \n  \n  \n    14 \n    2016-03-24 \n    1 \n    102 \n    101 \n    104 \n  \n  \n    1 \n    2015-07-01 \n    1 \n    103 \n    104 \n    101 \n  \n\n\n\n\n\nall_tables is a list which associates each of the filenames with the data read from that file, and in this example it corresponds to just the random A&E data.\nA snippet of the table is shown above.\nThe table has 6 columns, a patient id id, a date TODO update header at which the admission occured, an attendance_type, and three diagnosis categories, diagnosis_1, diagnosis_2, and diagnosis_3\nFrom the first row in this example we can see that patient 2 was admitted on the 16th of August 2016, they had 3 unique diagnoses, 104, 103, and 102. In the second row, the patient with the same id was admitted on the 28th July 2015, again they had 3 unique diagnosis, this time 101, 103, and 104."
  },
  {
    "objectID": "index.html#define-a-feature-to-extract",
    "href": "index.html#define-a-feature-to-extract",
    "title": "minisparra",
    "section": "2. Define a feature to extract",
    "text": "2. Define a feature to extract\nUsers then specify which features they would like to extract from which data. A simple specification file is given below.\nsource_file is a relative path to the data (in this case the random A&E data). transformation_type is the type of transformation which will take place. MINISPARRA has N different transformation types, X, Y, and Z. The user specifies how the data will be grouped via grouping_columns, which in this case is grouping by patient ID. An (optional) flag for what to do if data is absent is given - here it will be to report 0. output_feature_name defines the header for the column in the output dataframe which will contain the results of the specific transformation\n\nwriteLines(readLines(\"../example_spec/basic_ae_data.json\"))\n\n{\n  \"source_file\": [\"../example_data/random_ae_data.csv\"],\n  \"transformation_type\": [\"COUNT\"],\n  \"grouping_columns\": [\"id\"],\n  \"absent_data_flag\": [0],\n  \"output_feature_name\": [\"user_defined_name\"],\n  \"primary_filter\": {\n    \"type\": [\"AND\"],\n    \"subfilters\": {\n      \"subfilter_1\": {\n        \"column\": [\"attendance_category\"],\n        \"type\": [\"IN\"],\n        \"value\": [1]\n      },\n      \"subfilter_2\": {\n        \"column\": [\"diagnosis_1\"],\n        \"type\": [\"IN\"],\n        \"value\": [101]\n      }\n    }\n  }\n}\n\n\nThe primary_filter is AND, and there are two subfilters. The first subfilter - subfilter_1 specifies that the user wants to filter based on the attendance_category column of the random A&E-type data, and that the value that they are interested in is 1. That is only return results where the attendance_category is 1.\nThe second subfilter subfilter_2 filters based on the diagnosis_1 column and should only return results where diagnosis_1 is 101.\nAs the primary filter is AND, the resultant dataframe will be a count, grouped by patient id of A&E data with attendance_category = 1 AND diagnosos_1 = 101."
  },
  {
    "objectID": "index.html#extract-the-feature",
    "href": "index.html#extract-the-feature",
    "title": "minisparra",
    "section": "3. Extract the feature",
    "text": "3. Extract the feature\nNow we can use MINISPARRA to run the transformation pipeline. We specify the path to the data via all_table_filenames and the path the the specification via all_feature_json_filenames.\nWe then run the transformation which will produce the resultant dataframe.\n\nall_table_filenames &lt;- c(\"../example_data/random_ae_data.csv\")\nall_feature_json_filenames &lt;- c(\"../example_spec/basic_ae_data.json\")\n\ntf &lt;- transform(\n  all_table_filenames = all_table_filenames,\n  all_feature_json_filenames = all_feature_json_filenames\n)\n\ntf\n\n   id user_defined_name\n1   0                 2\n2   1                 1\n3   2                 1\n4   4                 2\n5   5                 1\n6   6                 2\n7   7                 5\n8   8                 2\n9  10                 1\n10 11                 1\n11 12                 1\n12 13                 1\n13 14                 1\n14 17                 2\n15 18                 1\n16 19                 2\n\n\nAbove is the resultant dataframe. We can see that patient with id 2 had 1 instance where their diagnosis_1 was 101, whereas patient with id 7 had 5 instances. It is also noted that patient with id 3 is not represented in this resultant dataframe. This could be because they had no occurences in the source data, or because they did not meet the filtering criteria of having an attendance_type of 1, and having a diagnosis_1 of 101."
  },
  {
    "objectID": "index.html#using-date-filtering",
    "href": "index.html#using-date-filtering",
    "title": "minisparra",
    "section": "4. Using date filtering",
    "text": "4. Using date filtering\nIn the above example the returned dataframe spanned all the dates over which the input data csv spanned. However this can be refined in the specification.\n\nwriteLines(readLines(\"quarto_examples/basic_ae_data_w_date.json\"))\n\n{\n  \"source_file\": [\"../example_data/random_ae_data.csv\"],\n  \"transformation_type\": [\"COUNT\"],\n  \"grouping_columns\": [\"id\"],\n  \"absent_data_flag\": [0],\n  \"output_feature_name\": [\"user_defined_name\"],\n  \"primary_filter\": {\n    \"type\": [\"AND\"],\n    \"subfilters\": {\n      \"subfilter_1\": {\n        \"column\": [\"attendance_category\"],\n        \"type\": [\"IN\"],\n        \"value\": [1]\n      },\n      \"subfilter_2\": {\n        \"column\": [\"diagnosis_1\"],\n        \"type\": [\"IN\"],\n        \"value\": [101]\n      },\n      \"subfilter_3\": {\n        \"column\": [\"time\"],\n        \"type\": [\"date_gt\"],\n        \"value\": [\"2015-12-31\"]\n      }\n    }\n  }\n}\n\n\nAn additonal subfilter has been added. Subfilter_3 is applied to the time column and filters based on dates that are greater than the 31st of December 2015. This means an event which happened on new-years-eve 2015 will not be included in the data, but events which occured on new-years-day 2016 would be.\nWe can again run this transformation pipeline with the same A&E data:\n\nall_table_filenames &lt;- c(\"../example_data/random_ae_data.csv\")\nall_feature_json_filenames &lt;- c(\"quarto_examples/basic_ae_data_w_date.json\")\n\ntf_w_date &lt;- transform(\n  all_table_filenames = all_table_filenames,\n  all_feature_json_filenames = all_feature_json_filenames\n)\n\ntf_w_date\n\n   id user_defined_name\n1   0                 2\n2   4                 1\n3   6                 1\n4   7                 5\n5   8                 1\n6  10                 1\n7  11                 1\n8  12                 1\n9  14                 1\n10 18                 1\n\n\nWe note that patient with id 2 is now missing from the resultant dataframe. This is expected as noted above patient 2 only had one instance where their diagnosis_1 was 101, and from inspection of the source data, we can see that it was on the 28th July 2015."
  },
  {
    "objectID": "index.html#using-or-filter",
    "href": "index.html#using-or-filter",
    "title": "minisparra",
    "section": "5. Using OR filter",
    "text": "5. Using OR filter\nWe can also expand the filters to return results if either diagnosis_1 or diagnosis_2 is 101. The specification file would look like this:\n\nwriteLines(readLines(\"quarto_examples/basic_ae_data_or_w_date.json\"))\n\n{\n  \"source_file\": [\"../example_data/random_ae_data.csv\"],\n  \"transformation_type\": [\"COUNT\"],\n  \"grouping_columns\": [\"id\"],\n  \"absent_data_flag\": [0],\n  \"output_feature_name\": [\"user_defined_name\"],\n  \"primary_filter\": {\n    \"type\": [\"AND\"],\n    \"subfilters\": {\n      \"subfilter_1\": {\n        \"column\": [\"attendance_category\"],\n        \"type\": [\"IN\"],\n        \"value\": [1]\n      },\n      \"subfilter_2\": {\n        \"type\": [\"OR\"],\n        \"subfilters\": {\n          \"subfilter_21\": {\n            \"column\": [\"diagnosis_1\"],\n            \"type\": [\"IN\"],\n            \"value\": [101]\n          },\n          \"subfilter_22\": {\n            \"column\": [\"diagnosis_2\"],\n            \"type\": [\"IN\"],\n            \"value\": [101]\n          }\n        }\n      },\n      \"subfilter_3\": {\n        \"column\": [\"time\"],\n        \"type\": [\"date_gt\"],\n        \"value\": [\"2015-12-31\"]\n      }\n    }\n  }\n}\n\n\nSubfilter 2 has now been defined as an OR type, and it contains it’s own subfilters, 21 and 22. subfilter_21 is a filter that diagnosis_1 column is 101, and subfilter_22 is a filter that diagnosis_2 column is 101. So the transformation will count an instance if either diagnosis_1 OR diagnosis_2 are 101. Subject to the other filters of the attendence_type being 1, and the date being later than 31st December 2015.\n\nall_table_filenames &lt;- c(\"../example_data/random_ae_data.csv\")\nall_feature_json_filenames &lt;- c(\"quarto_examples/basic_ae_data_or_w_date.json\")\n\ntf_or_w_date &lt;- transform(\n  all_table_filenames = all_table_filenames,\n  all_feature_json_filenames = all_feature_json_filenames\n)\n\ntf_or_w_date\n\n   id user_defined_name\n1   0                 4\n2   1                 1\n3   2                 2\n4   4                 1\n5   5                 1\n6   6                 1\n7   7                 5\n8   8                 2\n9  10                 2\n10 11                 1\n11 12                 2\n12 13                 3\n13 14                 2\n14 15                 1\n15 16                 1\n16 17                 1\n17 18                 2\n\n\nHere we see patient 14 has 2 instances of either diagnosis_1 or diagnosis_2 being 101 when the attendence_type is 1, and for dates later than 31st December 2015."
  },
  {
    "objectID": "index.html#logging",
    "href": "index.html#logging",
    "title": "minisparra",
    "section": "6.Logging",
    "text": "6.Logging\nLogging has also been introduced. By default the log is currently printed to the console. In order to see the log output in Quarto, we need to capture it in a temporary file.\nThe log is set up\n\nlibrary(logger)\ntmp &lt;- tempfile()\nlog_threshold(TRACE)\nlog_appender(appender_file(tmp))\n\nAnd the first basic example is run again. Reminder this is the specification”\n\nwriteLines(readLines(\"../example_spec/basic_ae_data.json\"))\n\n{\n  \"source_file\": [\"../example_data/random_ae_data.csv\"],\n  \"transformation_type\": [\"COUNT\"],\n  \"grouping_columns\": [\"id\"],\n  \"absent_data_flag\": [0],\n  \"output_feature_name\": [\"user_defined_name\"],\n  \"primary_filter\": {\n    \"type\": [\"AND\"],\n    \"subfilters\": {\n      \"subfilter_1\": {\n        \"column\": [\"attendance_category\"],\n        \"type\": [\"IN\"],\n        \"value\": [1]\n      },\n      \"subfilter_2\": {\n        \"column\": [\"diagnosis_1\"],\n        \"type\": [\"IN\"],\n        \"value\": [101]\n      }\n    }\n  }\n}\n\n\n\nall_table_filenames &lt;- c(\"../example_data/random_ae_data.csv\")\nall_feature_json_filenames &lt;- c(\"../example_spec/basic_ae_data.json\")\n\ntf &lt;- transform(\n  all_table_filenames = all_table_filenames,\n  all_feature_json_filenames = all_feature_json_filenames\n)\n\nCheck the log:\n\nreadLines(tmp)\n\n[1] \"TRACE [2024-02-29 10:10:58] context: featurise:../example_spec/basic_ae_data.json\"                                                                   \n[2] \"DEBUG [2024-02-29 10:10:58] Parsing nested filter\"                                                                                                   \n[3] \"DEBUG [2024-02-29 10:10:58] Parsing single filter\"                                                                                                   \n[4] \"DEBUG [2024-02-29 10:10:58] Parsing single filter\"                                                                                                   \n[5] \"TRACE [2024-02-29 10:10:58] context: featurise:../example_spec/basic_ae_data.json &gt; featurise_count\"                                                 \n[6] \"TRACE [2024-02-29 10:10:58] context: featurise:../example_spec/basic_ae_data.json &gt; featurise_count &gt; filter_and\"                                    \n[7] \"TRACE [2024-02-29 10:10:58] context: featurise:../example_spec/basic_ae_data.json &gt; featurise_count &gt; filter_and &gt; (1/2: subfilter_1) &gt; filter_basic\"\n[8] \"TRACE [2024-02-29 10:10:58] context: featurise:../example_spec/basic_ae_data.json &gt; featurise_count &gt; filter_and &gt; (2/2: subfilter_2) &gt; filter_basic\"\n[9] \"TRACE [2024-02-29 10:10:58] context: join_feature_tables\"                                                                                            \n\n\nLine 2 is when the primary filter is parsed. It has two subfilters so it is identified as a nested filter. The two subfilters (which themselves do not contain any subfilters) are then parsed, as reported on lines 3 and 4. Lines 5 - 8 give the context of what is being processed for debug purposes."
  },
  {
    "objectID": "index.html#errors-with-context",
    "href": "index.html#errors-with-context",
    "title": "minisparra",
    "section": "Errors with context",
    "text": "Errors with context\n\nExample 1\n\nlines &lt;- readLines(\"./ae2_data_wrong1.json\")\nlines[3]\n\n[1] \"  \\\"transformation_type\\\": [\\\"COUNT DRACULA\\\"],\"\n\n\n\ntransform(\n  all_table_filenames = all_table_filenames,\n  all_feature_json_filenames = \"./ae2_data_wrong1.json\")\n\nError: Unknown transformation type: count dracula\nContext:\n &gt; featurise:./ae2_data_wrong1.json\n\n\n\n\nExample 2\n\nlines &lt;- readLines(\"./ae2_data_wrong2.json\")\nlines[4]\n\n[1] \"  \\\"grouping_columns\\\": [\\\"this_column_doesnt_exist\\\"],\"\n\n\n\ntransform(\n  all_table_filenames = all_table_filenames,\n  all_feature_json_filenames = \"./ae2_data_wrong2.json\")\n\nError: Error in `rename()`:\n! Can't rename columns that don't exist.\n✖ Column `this_column_doesnt_exist` doesn't exist.\n\nContext:\n &gt; featurise:./ae2_data_wrong2.json\n &gt; featurise_count\n\n\n\n\nExample 3\n\nlines &lt;- readLines(\"./ae2_data_wrong3.json\")\nlines[11]\n\n[1] \"        \\\"column\\\": [\\\"at_ten_dance_category\\\"],\"\n\n\n\ntransform(\n  all_table_filenames = all_table_filenames,\n  all_feature_json_filenames = \"./ae2_data_wrong3.json\")\n\nError: Column 'at_ten_dance_category' not found in the table.\nContext:\n &gt; featurise:./ae2_data_wrong3.json\n &gt; featurise_count\n &gt; filter_and\n &gt; (1/2: subfilter_1)\n &gt; filter_basic"
  },
  {
    "objectID": "index.html#glossary",
    "href": "index.html#glossary",
    "title": "minisparra",
    "section": "Glossary",
    "text": "Glossary\n\nThe following comparison operators can be used for the type of filter\n\n“in” = %in%\n“lt” = &lt;\n“lt_eq” = &lt;=\n“gt” = &gt;\n“gt_eq” = &gt;=\n“date_in” = %in%\n“date_lt” = &lt;\n“date_lt_eq” = &lt;=\n“date_gt” = &gt;\n“date_gt_eq” = &gt;="
  }
]